{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb#scrollTo=StbPlIyKDP9E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_parquet(\"hf://datasets/data-is-better-together/fineweb-c/dan_Latn/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section01'></a>\n",
    "### Importing Python Libraries and preparing the environment\n",
    "\n",
    "At this step we will be importing the libraries and modules needed to run our script. Libraries are:\n",
    "* Pandas\n",
    "* Pytorch\n",
    "* Pytorch Utils for Dataset and Dataloader\n",
    "* Transformers\n",
    "* BERT Model and Tokenizer\n",
    "\n",
    "Followed by that we will preapre the device for GPU execeution. This configuration is needed if you want to leverage on onboard GPU.\n",
    "\n",
    "*I have included the code for TPU configuration, but commented it out. If you plan to use the TPU, please comment the GPU execution codes and uncomment the TPU ones to install the packages and define the device.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the transformers library and additional libraries if looking process\n",
    "\n",
    "!pip install -q transformers\n",
    "\n",
    "# Code for TPU packages install\n",
    "# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock ML Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from collections import Counter\n",
    "import ast\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section02'></a>\n",
    "### Importing and Pre-Processing the domain data\n",
    "\n",
    "We will be working with the data and preparing for fine tuning purposes.\n",
    "*Assuming that the `train.csv` is already downloaded, unzipped and saved in your `data` folder*\n",
    "\n",
    "* Import the file in a dataframe and give it the headers as per the documentation.\n",
    "* Taking the values of all the categories and coverting it into a list.\n",
    "* The list is appened as a new column and other columns are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Most_common_label(label_list):\n",
    "    label = Counter(label_list).most_common(1)[0][0]\n",
    "    inx = sort_order[label]  # Most frequent label\n",
    "    L = [0 for i in range(len(unique_labels))]\n",
    "    L[inx] = 1\n",
    "    return L\n",
    "\n",
    "\n",
    "def Soft_label(label_list):\n",
    "    numeric_annotations = [sort_order[label] for label in label_list]\n",
    "    return np.bincount(numeric_annotations, minlength=len(unique_labels)) / len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>educational_value_labels</th>\n",
       "      <th>problematic_content_label_present</th>\n",
       "      <th>Final_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>Cerfidan ApS er ISO-akkrediteret af DANAK til ...</td>\n",
       "      <td>[None, None, None]</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>Jeg kan varmt anbefale E-forsikringer. Jeg har...</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>Leder du efter gode kalendergaver til én, du h...</td>\n",
       "      <td>[None, None, None]</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Skønne fordele\\nI C&amp;S Universe, får du en rækk...</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>Allergica Bronci D6 - 10 x 1 ml\\nProduktbeskri...</td>\n",
       "      <td>[None, Minimal]</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "672  Cerfidan ApS er ISO-akkrediteret af DANAK til ...   \n",
       "662  Jeg kan varmt anbefale E-forsikringer. Jeg har...   \n",
       "425  Leder du efter gode kalendergaver til én, du h...   \n",
       "161  Skønne fordele\\nI C&S Universe, får du en rækk...   \n",
       "633  Allergica Bronci D6 - 10 x 1 ml\\nProduktbeskri...   \n",
       "\n",
       "    educational_value_labels  problematic_content_label_present  \\\n",
       "672       [None, None, None]                              False   \n",
       "662             [None, None]                              False   \n",
       "425       [None, None, None]                              False   \n",
       "161             [None, None]                              False   \n",
       "633          [None, Minimal]                              False   \n",
       "\n",
       "         Final_label  \n",
       "672  [0, 1, 0, 0, 0]  \n",
       "662  [0, 1, 0, 0, 0]  \n",
       "425  [0, 1, 0, 0, 0]  \n",
       "161  [0, 1, 0, 0, 0]  \n",
       "633  [0, 1, 0, 0, 0]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########\n",
    "DATASET = pd.read_parquet(\"hf://datasets/data-is-better-together/fineweb-c/dan_Latn/train-00000-of-00001.parquet\")\n",
    "PROBLEMATIC_CONTENT = False\n",
    "LABEL_FUNCTION = Most_common_label\n",
    "#########\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"text\"] = DATASET[\"text\"]\n",
    "df[\"educational_value_labels\"] = DATASET[\"educational_value_labels\"]\n",
    "df[\"problematic_content_label_present\"] = DATASET[\"problematic_content_label_present\"]\n",
    "\n",
    "\n",
    "# REMOVE PROBLEMATIC LABELS FROM DATASET\n",
    "df = df[df['problematic_content_label_present'] == PROBLEMATIC_CONTENT]\n",
    "\n",
    "unique_labels = df[\"educational_value_labels\"].explode().unique().tolist()\n",
    "sort_order = {\n",
    "    \"None\": unique_labels.index(\"None\"),\n",
    "    \"Minimal\": unique_labels.index(\"Minimal\"),\n",
    "    \"Basic\": unique_labels.index(\"Basic\"),\n",
    "    \"Good\": unique_labels.index(\"Good\"),\n",
    "    \"Excellent\": unique_labels.index(\"Excellent\"),\n",
    "}\n",
    "\n",
    "# Process Data labels\n",
    "df[\"Final_label\"] = df[\"educational_value_labels\"].apply(LABEL_FUNCTION)\n",
    "\n",
    "# Display sample rows\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Minimal', 'None', 'Basic', 'Excellent', 'Good']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame()\n",
    "new_df[\"text\"] = df[\"text\"]\n",
    "new_df[\"labels\"] = df[\"Final_label\"]\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section03'></a>\n",
    "### Preparing the Dataset and Dataloader\n",
    "\n",
    "We will start with defining few key variables that will be used later during the training/fine tuning stage.\n",
    "Followed by creation of CustomDataset class - This defines how the text is pre-processed before sending it to the neural network. We will also define the Dataloader that will feed  the data in batches to the neural network for suitable training and processing.\n",
    "Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the [docs at PyTorch](https://pytorch.org/docs/stable/data.html)\n",
    "\n",
    "#### *CustomDataset* Dataset Class\n",
    "- This class is defined to accept the `tokenizer`, `dataframe` and `max_length` as input and generate tokenized output and tags that is used by the BERT model for training.\n",
    "- We are using the BERT tokenizer to tokenize the data in the `comment_text` column of the dataframe.\n",
    "- The tokenizer uses the `encode_plus` method to perform tokenization and generate the necessary outputs, namely: `ids`, `attention_mask`, `token_type_ids`\n",
    "---\n",
    "- *This is the first difference between the distilbert and bert, where the tokenizer generates the token_type_ids in case of Bert*\n",
    "---\n",
    "- To read further into the tokenizer, [refer to this document](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer)\n",
    "- `targest` is the list of categories labled as `0` or `1` in the dataframe.\n",
    "- The *CustomDataset* class is used to create 2 datasets, for training and for validation.\n",
    "- *Training Dataset* is used to fine tune the model: **80% of the original data**\n",
    "- *Validation Dataset* is used to evaluate the performance of the model. The model has not seen this data during training.\n",
    "\n",
    "#### Dataloader\n",
    "- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled.\n",
    "- This control is achieved using the parameters such as `batch_size` and `max_len`.\n",
    "- Training and Validation dataloaders are used in the training and validation part of the flow respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128\n",
    "TRAIN_SIZE = 0.8\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (806, 2)\n",
      "TRAIN Dataset: (645, 2)\n",
      "TEST Dataset: (161, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = TRAIN_SIZE\n",
    "train_data=new_df.sample(frac=train_size,random_state=200)\n",
    "test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = MultiLabelDataset(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = MultiLabelDataset(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>Roger Federer slog verdens nummer to Novak Djo...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Selv om Louisa May Alcott er ofte forbundet me...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>Der er et stort projekt under opsejling hos De...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Hydrogen er verdens mindste og\\nmest effektive...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>En kæmpe hånd udspillede sig for kort tid side...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text           labels\n",
       "334  Roger Federer slog verdens nummer to Novak Djo...  [1, 0, 0, 0, 0]\n",
       "478  Selv om Louisa May Alcott er ofte forbundet me...  [1, 0, 0, 0, 0]\n",
       "528  Der er et stort projekt under opsejling hos De...  [1, 0, 0, 0, 0]\n",
       "88   Hydrogen er verdens mindste og\\nmest effektive...  [1, 0, 0, 0, 0]\n",
       "101  En kæmpe hånd udspillede sig for kort tid side...  [0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section04'></a>\n",
    "### Creating the Neural Network for Fine Tuning\n",
    "\n",
    "#### Neural Network\n",
    " - We will be creating a neural network with the `BERTClass`.\n",
    " - This network will have the `Bert` model.  Follwed by a `Droput` and `Linear Layer`. They are added for the purpose of **Regulariaztion** and **Classification** respectively.\n",
    " - In the forward loop, there are 2 output from the `BertModel` layer.\n",
    " - The second output `output_1` or called the `pooled output` is passed to the `Drop Out layer` and the subsequent output is given to the `Linear layer`.\n",
    " - Keep note the number of dimensions for `Linear Layer` is **6** because that is the total number of categories in which we are looking to classify our model.\n",
    " - The data will be fed to the `BertClass` as defined in the dataset.\n",
    " - Final layer outputs is what will be used to calcuate the loss and to determine the accuracy of models prediction.\n",
    " - We will initiate an instance of the network called `model`. This instance will be used for training and then to save the final trained model for future inference.\n",
    "\n",
    "#### Loss Function and Optimizer\n",
    " - The Loss is defined in the next cell as `loss_fn`.\n",
    " - As defined above, the loss function used will be a combination of Binary Cross Entropy which is implemented as [BCELogits Loss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss) in PyTorch\n",
    " - `Optimizer` is defined in the next cell.\n",
    " - `Optimizer` is used to update the weights of the neural network to improve its performance.\n",
    "\n",
    "#### Further Reading\n",
    "- You can refer to my [Pytorch Tutorials](https://github.com/abhimishra91/pytorch-tutorials) to get an intuition of Loss Function and Optimizer.\n",
    "- [Pytorch Documentation for Loss Function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "- [Pytorch Documentation for Optimizer](https://pytorch.org/docs/stable/optim.html)\n",
    "- Refer to the links provided on the top of the notebook to read more about `BertModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model.\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        #Change the secound val to the number of classes !!!!!!!!\n",
    "        self.l3 = torch.nn.Linear(768, len(unique_labels))\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "def accuracyTest(outputs, targets):\n",
    "    outputs = outputs.to(device, dtype=torch.int)\n",
    "    targets = targets.to(device, dtype=torch.int)\n",
    "\n",
    "    correct = torch.all(outputs == targets, dim=1)\n",
    "\n",
    "    correct_num = correct.sum().item()\n",
    "\n",
    "    accuracy = correct_num / len(targets)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies_pr_epoch = []\n",
    "val_accuracies_pr_epoch = []\n",
    "train_losses = []\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0  # Track total loss for the epoch\n",
    "    train_accuracies = []\n",
    "\n",
    "    for batch_idx,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        probs = torch.softmax(outputs, dim=1) # SOFTMAX (dim=1) or SIGMOID (), depends on how we interpret the multiclass\n",
    "    \n",
    "        max_indices = torch.argmax(probs, dim=1)\n",
    "        preds = torch.zeros_like(probs)\n",
    "        preds.scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "\n",
    "        print(preds)\n",
    "        print(targets)\n",
    "        batch_train_accuracy = accuracyTest(preds, targets)\n",
    "        train_accuracies.append(batch_train_accuracy)\n",
    "\n",
    "        print(batch_train_accuracy)\n",
    "        \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print loss every 10 batches (adjust as needed)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "    train_accuracies_pr_epoch.append(np.mean(train_accuracies)) # Save avrage accuracies for the epoch\n",
    "    avg_train_loss = total_loss / len(training_loader)\n",
    "    train_losses.append(avg_train_loss)  # Save average loss for the epoch\n",
    "    print(f'Epoch {epoch} Average Training Loss: {avg_train_loss}, Avrage Training Accuraccy: {np.mean(train_accuracies)}')\n",
    "\n",
    "val_accuracies = []\n",
    "val_f1_micro = []\n",
    "val_f1_macro = []\n",
    "test_accuracies_pr_epoch = []\n",
    "\n",
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    test_acc = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype=torch.long)\n",
    "            mask = data['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1) # SOFTMAX (dim=1) or SIGMOID (), depends on how we interpret the multiclass\n",
    "        \n",
    "            max_indices = torch.argmax(probs, dim=1)\n",
    "            preds = torch.zeros_like(probs)\n",
    "            preds.scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "\n",
    "            # print(preds)\n",
    "            # print(targets)\n",
    "            batch_test_acc = accuracyTest(preds, targets)\n",
    "            test_acc.append(batch_test_acc)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_accuracies_pr_epoch.append(np.mean(test_acc)) # Save avrage accuracies for the epoch\n",
    "    print(f'Epoch {epoch}, Avrage Test Accuraccy: {np.mean(test_acc)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Epoch Loop\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "    validation(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots\n",
    "epochs = list(range(EPOCHS))\n",
    "\n",
    "plt.plot(epochs, train_accuracies_pr_epoch, label='Train Accuracy')\n",
    "plt.plot(epochs, test_accuracies_pr_epoch, label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Train vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"learningcurve.png\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
