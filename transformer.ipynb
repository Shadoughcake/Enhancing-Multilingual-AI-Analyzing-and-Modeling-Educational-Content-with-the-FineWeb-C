{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb#scrollTo=StbPlIyKDP9E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>educational_value_labels</th>\n",
       "      <th>annotator_ids</th>\n",
       "      <th>problematic_content_label_present</th>\n",
       "      <th>problematic_content_label_agreement</th>\n",
       "      <th>language_names</th>\n",
       "      <th>language_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c293383d-cffc-4244-9e75-1cf1a25eb0cc</td>\n",
       "      <td>Er du en af de mange, der ikke kan se 6′eren d...</td>\n",
       "      <td>[None, None, None]</td>\n",
       "      <td>[a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 85ac8d5...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>dan_Latn</td>\n",
       "      <td>dan_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8ebfa2e5-fdb4-494a-817a-3a5b0fe09475</td>\n",
       "      <td>Behandlingsområder\\nHer kan du læse om regione...</td>\n",
       "      <td>[None, Minimal]</td>\n",
       "      <td>[a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 85ac8d5...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>dan_Latn</td>\n",
       "      <td>dan_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>917f3945-cec8-4cfe-8516-c72351489fa6</td>\n",
       "      <td>Schul Landskabsarkitekter arbejder med have- o...</td>\n",
       "      <td>[None, Minimal, None]</td>\n",
       "      <td>[95e4f810-6fe7-4bbd-8be6-a11b0d069c11, a0585a5...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>dan_Latn</td>\n",
       "      <td>dan_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8defb5be-a075-4709-9d61-db3632f61e13</td>\n",
       "      <td>Nyuddannede hurtigt i arbejde ad mælkevejen - ...</td>\n",
       "      <td>[Minimal, Minimal]</td>\n",
       "      <td>[a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 85ac8d5...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>dan_Latn</td>\n",
       "      <td>dan_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f71ad46e-cd61-45fb-8ab7-d82bdd5885f1</td>\n",
       "      <td>I onsdags var Kristian og jeg inde og kigge på...</td>\n",
       "      <td>[None, ❗ Problematic Content ❗, None]</td>\n",
       "      <td>[a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 9987848...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>dan_Latn</td>\n",
       "      <td>dan_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>da2d09ad-af5d-4690-8474-72152a61d4f8</td>\n",
       "      <td>Sorte singler datingYozshudal\\nFind kærlighede...</td>\n",
       "      <td>[❗ Problematic Content ❗, ❗ Problematic Conten...</td>\n",
       "      <td>[a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 85ac8d5...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>dan_Latn</td>\n",
       "      <td>dan_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>74f77a6f-f30e-48d1-b90a-f1673bd2787a</td>\n",
       "      <td>Datjng er mange gode grunde til, at kvinder, a...</td>\n",
       "      <td>[❗ Problematic Content ❗, None, ❗ Problematic ...</td>\n",
       "      <td>[a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, b98b014...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>dan_Latn</td>\n",
       "      <td>dan_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>cd146702-e112-415a-9017-83ebaed7390a</td>\n",
       "      <td>gay aroma thai massage århus porno ældre herre...</td>\n",
       "      <td>[❗ Problematic Content ❗, ❗ Problematic Conten...</td>\n",
       "      <td>[a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, b98b014...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>dan_Latn</td>\n",
       "      <td>dan_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>a7d17d2b-90ec-4bc5-b6a7-ceb66c5d76e2</td>\n",
       "      <td>Borgerservice genåbner i dag\\nBorgerservice i ...</td>\n",
       "      <td>[Minimal, Minimal]</td>\n",
       "      <td>[a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 85ac8d5...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>dan_Latn</td>\n",
       "      <td>dan_Latn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>4ca17bc0-6e0e-47d2-b47c-fd28462b144f</td>\n",
       "      <td>|Titel :||Oathmark: Bring and Battle|\\n|Regels...</td>\n",
       "      <td>[Minimal, Minimal]</td>\n",
       "      <td>[a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 85ac8d5...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>dan_Latn</td>\n",
       "      <td>dan_Latn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id  \\\n",
       "0    c293383d-cffc-4244-9e75-1cf1a25eb0cc   \n",
       "1    8ebfa2e5-fdb4-494a-817a-3a5b0fe09475   \n",
       "2    917f3945-cec8-4cfe-8516-c72351489fa6   \n",
       "3    8defb5be-a075-4709-9d61-db3632f61e13   \n",
       "4    f71ad46e-cd61-45fb-8ab7-d82bdd5885f1   \n",
       "..                                    ...   \n",
       "995  da2d09ad-af5d-4690-8474-72152a61d4f8   \n",
       "996  74f77a6f-f30e-48d1-b90a-f1673bd2787a   \n",
       "997  cd146702-e112-415a-9017-83ebaed7390a   \n",
       "998  a7d17d2b-90ec-4bc5-b6a7-ceb66c5d76e2   \n",
       "999  4ca17bc0-6e0e-47d2-b47c-fd28462b144f   \n",
       "\n",
       "                                                  text  \\\n",
       "0    Er du en af de mange, der ikke kan se 6′eren d...   \n",
       "1    Behandlingsområder\\nHer kan du læse om regione...   \n",
       "2    Schul Landskabsarkitekter arbejder med have- o...   \n",
       "3    Nyuddannede hurtigt i arbejde ad mælkevejen - ...   \n",
       "4    I onsdags var Kristian og jeg inde og kigge på...   \n",
       "..                                                 ...   \n",
       "995  Sorte singler datingYozshudal\\nFind kærlighede...   \n",
       "996  Datjng er mange gode grunde til, at kvinder, a...   \n",
       "997  gay aroma thai massage århus porno ældre herre...   \n",
       "998  Borgerservice genåbner i dag\\nBorgerservice i ...   \n",
       "999  |Titel :||Oathmark: Bring and Battle|\\n|Regels...   \n",
       "\n",
       "                              educational_value_labels  \\\n",
       "0                                   [None, None, None]   \n",
       "1                                      [None, Minimal]   \n",
       "2                                [None, Minimal, None]   \n",
       "3                                   [Minimal, Minimal]   \n",
       "4                [None, ❗ Problematic Content ❗, None]   \n",
       "..                                                 ...   \n",
       "995  [❗ Problematic Content ❗, ❗ Problematic Conten...   \n",
       "996  [❗ Problematic Content ❗, None, ❗ Problematic ...   \n",
       "997  [❗ Problematic Content ❗, ❗ Problematic Conten...   \n",
       "998                                 [Minimal, Minimal]   \n",
       "999                                 [Minimal, Minimal]   \n",
       "\n",
       "                                         annotator_ids  \\\n",
       "0    [a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 85ac8d5...   \n",
       "1    [a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 85ac8d5...   \n",
       "2    [95e4f810-6fe7-4bbd-8be6-a11b0d069c11, a0585a5...   \n",
       "3    [a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 85ac8d5...   \n",
       "4    [a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 9987848...   \n",
       "..                                                 ...   \n",
       "995  [a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 85ac8d5...   \n",
       "996  [a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, b98b014...   \n",
       "997  [a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, b98b014...   \n",
       "998  [a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 85ac8d5...   \n",
       "999  [a0585a5c-b72f-4c3a-a2a3-17e8e0b4ea4f, 85ac8d5...   \n",
       "\n",
       "     problematic_content_label_present  problematic_content_label_agreement  \\\n",
       "0                                False                             1.000000   \n",
       "1                                False                             1.000000   \n",
       "2                                False                             1.000000   \n",
       "3                                False                             1.000000   \n",
       "4                                 True                             0.333333   \n",
       "..                                 ...                                  ...   \n",
       "995                               True                             1.000000   \n",
       "996                               True                             0.666667   \n",
       "997                               True                             1.000000   \n",
       "998                              False                             1.000000   \n",
       "999                              False                             1.000000   \n",
       "\n",
       "    language_names language_code  \n",
       "0         dan_Latn      dan_Latn  \n",
       "1         dan_Latn      dan_Latn  \n",
       "2         dan_Latn      dan_Latn  \n",
       "3         dan_Latn      dan_Latn  \n",
       "4         dan_Latn      dan_Latn  \n",
       "..             ...           ...  \n",
       "995       dan_Latn      dan_Latn  \n",
       "996       dan_Latn      dan_Latn  \n",
       "997       dan_Latn      dan_Latn  \n",
       "998       dan_Latn      dan_Latn  \n",
       "999       dan_Latn      dan_Latn  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_parquet(\"hf://datasets/data-is-better-together/fineweb-c/dan_Latn/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section01'></a>\n",
    "### Importing Python Libraries and preparing the environment\n",
    "\n",
    "At this step we will be importing the libraries and modules needed to run our script. Libraries are:\n",
    "* Pandas\n",
    "* Pytorch\n",
    "* Pytorch Utils for Dataset and Dataloader\n",
    "* Transformers\n",
    "* BERT Model and Tokenizer\n",
    "\n",
    "Followed by that we will preapre the device for GPU execeution. This configuration is needed if you want to leverage on onboard GPU.\n",
    "\n",
    "*I have included the code for TPU configuration, but commented it out. If you plan to use the TPU, please comment the GPU execution codes and uncomment the TPU ones to install the packages and define the device.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the transformers library and additional libraries if looking process\n",
    "\n",
    "!pip install -q transformers\n",
    "\n",
    "# Code for TPU packages install\n",
    "# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stock ML Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section02'></a>\n",
    "### Importing and Pre-Processing the domain data\n",
    "\n",
    "We will be working with the data and preparing for fine tuning purposes.\n",
    "*Assuming that the `train.csv` is already downloaded, unzipped and saved in your `data` folder*\n",
    "\n",
    "* Import the file in a dataframe and give it the headers as per the documentation.\n",
    "* Taking the values of all the categories and coverting it into a list.\n",
    "* The list is appened as a new column and other columns are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Most_common_label(label_list):\n",
    "    label = Counter(label_list).most_common(1)[0][0]\n",
    "    inx = sort_order[label]  # Most frequent label\n",
    "    L = [0 for i in range(len(unique_labels))]\n",
    "    L[inx] = 1\n",
    "    return L\n",
    "\n",
    "\n",
    "def Soft_label(label_list):\n",
    "    numeric_annotations = [sort_order[label] for label in label_list]\n",
    "    return np.bincount(numeric_annotations, minlength=len(unique_labels)) / len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>educational_value_labels</th>\n",
       "      <th>problematic_content_label_present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>I dagene fra onsdag den 12. til og med lørdag ...</td>\n",
       "      <td>[Educational]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Nyheder\\n22.08.14 - Tillykke til vinderne af P...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>Lad dit barns kreativitet blomstre\\nGiv dit ba...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>I Hedensted Kommune betaler vi for at bruge la...</td>\n",
       "      <td>[Educational]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>Efter hovedværket 'De ti skud' i 2004 bedyrede...</td>\n",
       "      <td>[Educational]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "114  I dagene fra onsdag den 12. til og med lørdag ...   \n",
       "308  Nyheder\\n22.08.14 - Tillykke til vinderne af P...   \n",
       "466  Lad dit barns kreativitet blomstre\\nGiv dit ba...   \n",
       "197  I Hedensted Kommune betaler vi for at bruge la...   \n",
       "662  Efter hovedværket 'De ti skud' i 2004 bedyrede...   \n",
       "\n",
       "    educational_value_labels  problematic_content_label_present  \n",
       "114            [Educational]                              False  \n",
       "308                   [None]                              False  \n",
       "466                   [None]                              False  \n",
       "197            [Educational]                              False  \n",
       "662            [Educational]                              False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>educational_value_labels</th>\n",
       "      <th>problematic_content_label_present</th>\n",
       "      <th>Final_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>Her i vores (ufrivillige) træningsløbspause ka...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>For rejsende der gerne vil opleve synet og lyd...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Baser dig selv i det travle centrum af Milanos...</td>\n",
       "      <td>[Educational]</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>Martin Hvidt\\nLektor, dr.phil.\\nBiografi\\nMart...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>Marcelos søn med hattrick i El Derbi\\nTil alle...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "513  Her i vores (ufrivillige) træningsløbspause ka...   \n",
       "22   For rejsende der gerne vil opleve synet og lyd...   \n",
       "295  Baser dig selv i det travle centrum af Milanos...   \n",
       "794  Martin Hvidt\\nLektor, dr.phil.\\nBiografi\\nMart...   \n",
       "792  Marcelos søn med hattrick i El Derbi\\nTil alle...   \n",
       "\n",
       "    educational_value_labels  problematic_content_label_present Final_label  \n",
       "513                   [None]                              False      [1, 0]  \n",
       "22                    [None]                              False      [1, 0]  \n",
       "295            [Educational]                              False      [0, 1]  \n",
       "794                   [None]                              False      [1, 0]  \n",
       "792                   [None]                              False      [1, 0]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########\n",
    "DATASET = pd.read_parquet(\"hf://datasets/data-is-better-together/fineweb-c/dan_Latn/train-00000-of-00001.parquet\")\n",
    "PROBLEMATIC_CONTENT = False\n",
    "LABEL_FUNCTION = Most_common_label\n",
    "ex_Data_path = \"fineweb2_data.csv\"\n",
    "Binary_Classification = True \n",
    "#########\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"text\"] = DATASET[\"text\"]\n",
    "df[\"educational_value_labels\"] = DATASET[\"educational_value_labels\"]\n",
    "df[\"problematic_content_label_present\"] = DATASET[\"problematic_content_label_present\"]\n",
    "\n",
    "extra_df = pd.read_csv(ex_Data_path)\n",
    "extra_df = extra_df.rename(columns={\"label\": \"educational_value_labels\"})\n",
    "extra_df['educational_value_labels'] = extra_df['educational_value_labels'].apply(ast.literal_eval)\n",
    "extra_df['problematic_content_label_present'] = False\n",
    "\n",
    "df = pd.concat([df, extra_df], ignore_index=True)\n",
    "\n",
    "# REMOVE PROBLEMATIC LABELS FROM DATASET\n",
    "df = df[df['problematic_content_label_present'] == PROBLEMATIC_CONTENT]\n",
    "\n",
    "unique_labels = df[\"educational_value_labels\"].explode().unique().tolist()\n",
    "sort_order = {\n",
    "    \"None\": unique_labels.index(\"None\"),\n",
    "    \"Minimal\": unique_labels.index(\"Minimal\"),\n",
    "    \"Basic\": unique_labels.index(\"Basic\"),\n",
    "    \"Good\": unique_labels.index(\"Good\"),\n",
    "    \"Excellent\": unique_labels.index(\"Excellent\"),\n",
    "}\n",
    "\n",
    "# Convert Multi-Labels to Binary Labels\n",
    "if Binary_Classification:\n",
    "    unique_labels = [\"None\", \"Educational\"]\n",
    "    sort_order = {\n",
    "        \"None\": 0,\n",
    "        \"Educational\": 1\n",
    "    }\n",
    "    df[\"educational_value_labels\"] = df[\"educational_value_labels\"].apply(\n",
    "        lambda x: [\"Educational\"] if \"None\" not in x else [\"None\"])\n",
    "\n",
    "display(df.sample(5))\n",
    "\n",
    "# Process Data labels\n",
    "df[\"Final_label\"] = df[\"educational_value_labels\"].apply(LABEL_FUNCTION)\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None', 'Educational']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame()\n",
    "new_df[\"text\"] = df[\"text\"]\n",
    "new_df[\"labels\"] = df[\"Final_label\"]\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section03'></a>\n",
    "### Preparing the Dataset and Dataloader\n",
    "\n",
    "We will start with defining few key variables that will be used later during the training/fine tuning stage.\n",
    "Followed by creation of CustomDataset class - This defines how the text is pre-processed before sending it to the neural network. We will also define the Dataloader that will feed  the data in batches to the neural network for suitable training and processing.\n",
    "Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the [docs at PyTorch](https://pytorch.org/docs/stable/data.html)\n",
    "\n",
    "#### *CustomDataset* Dataset Class\n",
    "- This class is defined to accept the `tokenizer`, `dataframe` and `max_length` as input and generate tokenized output and tags that is used by the BERT model for training.\n",
    "- We are using the BERT tokenizer to tokenize the data in the `comment_text` column of the dataframe.\n",
    "- The tokenizer uses the `encode_plus` method to perform tokenization and generate the necessary outputs, namely: `ids`, `attention_mask`, `token_type_ids`\n",
    "---\n",
    "- *This is the first difference between the distilbert and bert, where the tokenizer generates the token_type_ids in case of Bert*\n",
    "---\n",
    "- To read further into the tokenizer, [refer to this document](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer)\n",
    "- `targest` is the list of categories labled as `0` or `1` in the dataframe.\n",
    "- The *CustomDataset* class is used to create 2 datasets, for training and for validation.\n",
    "- *Training Dataset* is used to fine tune the model: **80% of the original data**\n",
    "- *Validation Dataset* is used to evaluate the performance of the model. The model has not seen this data during training.\n",
    "\n",
    "#### Dataloader\n",
    "- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled.\n",
    "- This control is achieved using the parameters such as `batch_size` and `max_len`.\n",
    "- Training and Validation dataloaders are used in the training and validation part of the flow respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128\n",
    "TRAIN_SIZE = 0.8\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length', # New: replaces deprecated pad_to_max_length (pad_to_max_length=True,)\n",
    "            truncation=True,      # explicitly set truncation\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text           labels\n",
      "0     Er du en af de mange, der ikke kan se 6′eren d...  [1, 0, 0, 0, 0]\n",
      "1     Behandlingsområder\\nHer kan du læse om regione...  [1, 0, 0, 0, 0]\n",
      "2     Schul Landskabsarkitekter arbejder med have- o...  [1, 0, 0, 0, 0]\n",
      "3     Nyuddannede hurtigt i arbejde ad mælkevejen - ...  [0, 1, 0, 0, 0]\n",
      "5     Beliggenhed: Kina > Beijing > Beijing West Rai...  [1, 0, 0, 0, 0]\n",
      "...                                                 ...              ...\n",
      "1008  Anden renses for indmad og skylles. Gnides her...  [0, 0, 0, 1, 0]\n",
      "1009  Lusen er et snyltedyr, dvs. et lille insekt, d...  [0, 0, 0, 0, 1]\n",
      "1010  \\n\\nHvad er Ordblindhed (dysleksi)\\n|Forfatter...  [0, 0, 0, 0, 1]\n",
      "1011  skum, masse af gasfyldte bobler med elastiske ...  [0, 0, 0, 1, 0]\n",
      "1012  genteknologi, betegnelse for en række metoder,...  [0, 0, 0, 0, 1]\n",
      "\n",
      "[819 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (819, 2)\n",
      "TRAIN Dataset: (655, 2)\n",
      "TEST Dataset: (164, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = TRAIN_SIZE\n",
    "train_data=new_df.sample(frac=train_size,random_state=200)\n",
    "test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = MultiLabelDataset(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = MultiLabelDataset(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check class distribution in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training set:\n",
      "labels\n",
      "0    374\n",
      "1    215\n",
      "2     49\n",
      "3     14\n",
      "4      3\n",
      "Name: count, dtype: int64\n",
      "Class distribution in training set:\n",
      "labels\n",
      "0    97\n",
      "1    45\n",
      "2    15\n",
      "3     6\n",
      "4     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### Check class distribution\n",
    "\n",
    "def class_distribution(df):\n",
    "    idx = df['labels'].apply(lambda x: np.argmax(x))\n",
    "\n",
    "    # Get distribution\n",
    "    class_counts = idx.value_counts().sort_index()\n",
    "\n",
    "    print(\"Class distribution in training set:\")\n",
    "    print(class_counts)\n",
    "\n",
    "class_distribution(train_data)\n",
    "class_distribution(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>En helt enkel højtaler til tablets uden behov ...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Fuglehus til haven – Find det perfekte foderbr...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>PSYKIATRI Der er flere grunde til at være glad...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>FODBOLD: Der er ofte udfordringer for Brabrand...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>Sådan bliver du dig selv3. april 2011\\nLev i n...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text           labels\n",
       "53   En helt enkel højtaler til tablets uden behov ...  [1, 0, 0, 0, 0]\n",
       "159  Fuglehus til haven – Find det perfekte foderbr...  [1, 0, 0, 0, 0]\n",
       "262  PSYKIATRI Der er flere grunde til at være glad...  [0, 1, 0, 0, 0]\n",
       "584  FODBOLD: Der er ofte udfordringer for Brabrand...  [1, 0, 0, 0, 0]\n",
       "357  Sådan bliver du dig selv3. april 2011\\nLev i n...  [1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1470f0110>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section04'></a>\n",
    "### Creating the Neural Network for Fine Tuning\n",
    "\n",
    "#### Neural Network\n",
    " - We will be creating a neural network with the `BERTClass`.\n",
    " - This network will have the `Bert` model.  Follwed by a `Droput` and `Linear Layer`. They are added for the purpose of **Regulariaztion** and **Classification** respectively.\n",
    " - In the forward loop, there are 2 output from the `BertModel` layer.\n",
    " - The second output `output_1` or called the `pooled output` is passed to the `Drop Out layer` and the subsequent output is given to the `Linear layer`.\n",
    " - Keep note the number of dimensions for `Linear Layer` is **6** because that is the total number of categories in which we are looking to classify our model.\n",
    " - The data will be fed to the `BertClass` as defined in the dataset.\n",
    " - Final layer outputs is what will be used to calcuate the loss and to determine the accuracy of models prediction.\n",
    " - We will initiate an instance of the network called `model`. This instance will be used for training and then to save the final trained model for future inference.\n",
    "\n",
    "#### Loss Function and Optimizer\n",
    " - The Loss is defined in the next cell as `loss_fn`.\n",
    " - As defined above, the loss function used will be a combination of Binary Cross Entropy which is implemented as [BCELogits Loss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss) in PyTorch\n",
    " - `Optimizer` is defined in the next cell.\n",
    " - `Optimizer` is used to update the weights of the neural network to improve its performance.\n",
    "\n",
    "#### Further Reading\n",
    "- You can refer to my [Pytorch Tutorials](https://github.com/abhimishra91/pytorch-tutorials) to get an intuition of Loss Function and Optimizer.\n",
    "- [Pytorch Documentation for Loss Function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "- [Pytorch Documentation for Optimizer](https://pytorch.org/docs/stable/optim.html)\n",
    "- Refer to the links provided on the top of the notebook to read more about `BertModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model.\n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        #Change the secound val to the number of classes !!!!!!!!\n",
    "        self.l3 = torch.nn.Linear(768, len(unique_labels))\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss, accuracy, (weighted loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded labels to class indices\n",
    "class_indices = [label.index(1) for label in train_data['labels']]\n",
    "class_counts = torch.bincount(torch.tensor(class_indices))\n",
    "class_weights = 1.0 / class_counts.float()  # inverse frequency\n",
    "class_weights = class_weights / class_weights.sum()  # normalization\n",
    "\n",
    "# Define loss_fn with weights\n",
    "weighted_loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    # return torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "    \n",
    "    target_indices = torch.argmax(targets, dim=1)  # [batch_size]\n",
    "    return weighted_loss_fn(outputs, target_indices)\n",
    "\n",
    "def accuracyTest(outputs, targets):\n",
    "    outputs = outputs.to(device, dtype=torch.int)\n",
    "    targets = targets.to(device, dtype=torch.int)\n",
    "\n",
    "    correct = torch.all(outputs == targets, dim=1)\n",
    "\n",
    "    correct_num = correct.sum().item()\n",
    "\n",
    "    accuracy = correct_num / len(targets)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies_pr_epoch = []\n",
    "test_accuracies_pr_epoch = []\n",
    "train_losses = []\n",
    "confusion_matrix_pr_epoch = []\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0  # Track total loss for the epoch\n",
    "    train_accuracies = []\n",
    "\n",
    "    for batch_idx,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        probs = torch.softmax(outputs, dim=1) # SOFTMAX (dim=1) or SIGMOID (), depends on how we interpret the multiclass\n",
    "    \n",
    "        max_indices = torch.argmax(probs, dim=1)\n",
    "        preds = torch.zeros_like(probs)\n",
    "        preds.scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "\n",
    "        # print(preds)\n",
    "        # print(targets)\n",
    "        batch_train_accuracy = accuracyTest(preds, targets)\n",
    "        train_accuracies.append(batch_train_accuracy)\n",
    "\n",
    "        print(f\"Batch {batch_idx} Train Accuracy:\", batch_train_accuracy)\n",
    "        \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print loss every 10 batches (adjust as needed)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "    train_accuracies_pr_epoch.append(np.mean(train_accuracies)) # Save avrage accuracies for the epoch\n",
    "    avg_train_loss = total_loss / len(training_loader)\n",
    "    train_losses.append(avg_train_loss)  # Save average loss for the epoch\n",
    "    print(f'Epoch {epoch} Average Training Loss: {avg_train_loss}, Avrage Training Accuraccy: {np.mean(train_accuracies)}')\n",
    "\n",
    "\n",
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    confusion_matrix = np.zeros((len(unique_labels), len(unique_labels)))\n",
    "\n",
    "    test_acc = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype=torch.long)\n",
    "            mask = data['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1) # SOFTMAX (dim=1) or SIGMOID (), depends on how we interpret the multiclass\n",
    "        \n",
    "            max_indices = torch.argmax(probs, dim=1)\n",
    "            preds = torch.zeros_like(probs)\n",
    "            preds.scatter_(1, max_indices.unsqueeze(1), 1)\n",
    "\n",
    "            print(preds)\n",
    "            print(targets)\n",
    "\n",
    "            # Print confusion matrix for each sample\n",
    "            for i, (target, pred) in enumerate(zip(targets, preds)):\n",
    "                true_class = target.argmax().item()\n",
    "                predicted_class = pred.argmax().item()\n",
    "                \n",
    "                # Print per-sample prediction\n",
    "                print(f\"Sample {i}: true class = {true_class}, predicted = {predicted_class}\")\n",
    "                # Update confusion matrix\n",
    "                confusion_matrix[true_class, predicted_class] += 1\n",
    "            \n",
    "\n",
    "            batch_test_acc = accuracyTest(preds, targets)\n",
    "            test_acc.append(batch_test_acc)\n",
    "            \n",
    "    # Print confusion matrix for the epoch        \n",
    "    print(confusion_matrix)\n",
    "    confusion_matrix_pr_epoch.append(confusion_matrix)  # Save confusion matrix for the epoch\n",
    "    # Calculate metrics\n",
    "    test_accuracies_pr_epoch.append(np.mean(test_acc)) # Save avrage accuracies for the epoch\n",
    "    print(f'Epoch {epoch}, Avrage Test Accuraccy: {np.mean(test_acc)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 Train Accuracy: 0.25\n",
      "Epoch: 0, Batch: 0, Loss: 0.47275489568710327\n",
      "Batch 1 Train Accuracy: 0.25\n",
      "Batch 2 Train Accuracy: 0.75\n",
      "Batch 3 Train Accuracy: 0.5\n",
      "Batch 4 Train Accuracy: 0.25\n",
      "Batch 5 Train Accuracy: 0.5\n",
      "Batch 6 Train Accuracy: 0.25\n",
      "Batch 7 Train Accuracy: 0.0\n",
      "Batch 8 Train Accuracy: 0.75\n",
      "Batch 9 Train Accuracy: 0.5\n",
      "Batch 10 Train Accuracy: 1.0\n",
      "Epoch: 0, Batch: 10, Loss: 0.2851070463657379\n",
      "Batch 11 Train Accuracy: 0.5\n",
      "Batch 12 Train Accuracy: 0.75\n",
      "Batch 13 Train Accuracy: 0.25\n",
      "Batch 14 Train Accuracy: 1.0\n",
      "Batch 15 Train Accuracy: 0.5\n",
      "Batch 16 Train Accuracy: 0.5\n",
      "Batch 17 Train Accuracy: 1.0\n",
      "Batch 18 Train Accuracy: 0.75\n",
      "Batch 19 Train Accuracy: 0.75\n",
      "Batch 20 Train Accuracy: 0.5\n",
      "Epoch: 0, Batch: 20, Loss: 0.4490510821342468\n",
      "Batch 21 Train Accuracy: 0.75\n",
      "Batch 22 Train Accuracy: 0.5\n",
      "Batch 23 Train Accuracy: 0.75\n",
      "Batch 24 Train Accuracy: 0.5\n",
      "Batch 25 Train Accuracy: 0.5\n",
      "Batch 26 Train Accuracy: 0.5\n",
      "Batch 27 Train Accuracy: 0.5\n",
      "Batch 28 Train Accuracy: 0.0\n",
      "Batch 29 Train Accuracy: 0.5\n",
      "Batch 30 Train Accuracy: 0.75\n",
      "Epoch: 0, Batch: 30, Loss: 0.27851325273513794\n",
      "Batch 31 Train Accuracy: 0.75\n",
      "Batch 32 Train Accuracy: 0.5\n",
      "Batch 33 Train Accuracy: 0.5\n",
      "Batch 34 Train Accuracy: 0.75\n",
      "Batch 35 Train Accuracy: 0.5\n",
      "Batch 36 Train Accuracy: 0.25\n",
      "Batch 37 Train Accuracy: 0.5\n",
      "Batch 38 Train Accuracy: 0.5\n",
      "Batch 39 Train Accuracy: 0.5\n",
      "Batch 40 Train Accuracy: 0.75\n",
      "Epoch: 0, Batch: 40, Loss: 0.3947913646697998\n",
      "Batch 41 Train Accuracy: 0.5\n",
      "Batch 42 Train Accuracy: 0.5\n",
      "Batch 43 Train Accuracy: 0.0\n",
      "Batch 44 Train Accuracy: 0.5\n",
      "Batch 45 Train Accuracy: 1.0\n",
      "Batch 46 Train Accuracy: 0.75\n",
      "Batch 47 Train Accuracy: 0.5\n",
      "Batch 48 Train Accuracy: 0.25\n",
      "Batch 49 Train Accuracy: 0.5\n",
      "Batch 50 Train Accuracy: 0.25\n",
      "Epoch: 0, Batch: 50, Loss: 0.32266223430633545\n",
      "Batch 51 Train Accuracy: 0.5\n",
      "Batch 52 Train Accuracy: 0.5\n",
      "Batch 53 Train Accuracy: 0.25\n",
      "Batch 54 Train Accuracy: 0.5\n",
      "Batch 55 Train Accuracy: 0.5\n",
      "Batch 56 Train Accuracy: 1.0\n",
      "Batch 57 Train Accuracy: 0.5\n",
      "Batch 58 Train Accuracy: 0.5\n",
      "Batch 59 Train Accuracy: 0.75\n",
      "Batch 60 Train Accuracy: 0.5\n",
      "Epoch: 0, Batch: 60, Loss: 0.4386593699455261\n",
      "Batch 61 Train Accuracy: 0.5\n",
      "Batch 62 Train Accuracy: 0.5\n",
      "Batch 63 Train Accuracy: 0.5\n",
      "Batch 64 Train Accuracy: 0.5\n",
      "Batch 65 Train Accuracy: 0.5\n",
      "Batch 66 Train Accuracy: 0.0\n",
      "Batch 67 Train Accuracy: 0.75\n",
      "Batch 68 Train Accuracy: 1.0\n",
      "Batch 69 Train Accuracy: 0.25\n",
      "Batch 70 Train Accuracy: 0.5\n",
      "Epoch: 0, Batch: 70, Loss: 0.315367728471756\n",
      "Batch 71 Train Accuracy: 0.75\n",
      "Batch 72 Train Accuracy: 1.0\n",
      "Batch 73 Train Accuracy: 0.25\n",
      "Batch 74 Train Accuracy: 0.75\n",
      "Batch 75 Train Accuracy: 0.75\n",
      "Batch 76 Train Accuracy: 0.75\n",
      "Batch 77 Train Accuracy: 0.75\n",
      "Batch 78 Train Accuracy: 0.5\n",
      "Batch 79 Train Accuracy: 0.75\n",
      "Batch 80 Train Accuracy: 0.75\n",
      "Epoch: 0, Batch: 80, Loss: 0.2786124646663666\n",
      "Batch 81 Train Accuracy: 0.75\n",
      "Batch 82 Train Accuracy: 0.5\n",
      "Batch 83 Train Accuracy: 0.75\n",
      "Batch 84 Train Accuracy: 0.5\n",
      "Batch 85 Train Accuracy: 0.25\n",
      "Batch 86 Train Accuracy: 0.5\n",
      "Batch 87 Train Accuracy: 0.5\n",
      "Batch 88 Train Accuracy: 0.25\n",
      "Batch 89 Train Accuracy: 0.75\n",
      "Batch 90 Train Accuracy: 0.5\n",
      "Epoch: 0, Batch: 90, Loss: 0.34697210788726807\n",
      "Batch 91 Train Accuracy: 0.75\n",
      "Batch 92 Train Accuracy: 0.5\n",
      "Batch 93 Train Accuracy: 0.25\n",
      "Batch 94 Train Accuracy: 0.25\n",
      "Batch 95 Train Accuracy: 0.5\n",
      "Batch 96 Train Accuracy: 0.5\n",
      "Batch 97 Train Accuracy: 0.75\n",
      "Batch 98 Train Accuracy: 0.75\n",
      "Batch 99 Train Accuracy: 0.75\n",
      "Batch 100 Train Accuracy: 0.75\n",
      "Epoch: 0, Batch: 100, Loss: 0.2958511412143707\n",
      "Batch 101 Train Accuracy: 0.5\n",
      "Batch 102 Train Accuracy: 0.75\n",
      "Batch 103 Train Accuracy: 0.75\n",
      "Batch 104 Train Accuracy: 0.75\n",
      "Batch 105 Train Accuracy: 0.75\n",
      "Batch 106 Train Accuracy: 0.5\n",
      "Batch 107 Train Accuracy: 0.5\n",
      "Batch 108 Train Accuracy: 0.75\n",
      "Batch 109 Train Accuracy: 0.5\n",
      "Batch 110 Train Accuracy: 0.75\n",
      "Epoch: 0, Batch: 110, Loss: 0.3412869870662689\n",
      "Batch 111 Train Accuracy: 0.25\n",
      "Batch 112 Train Accuracy: 0.5\n",
      "Batch 113 Train Accuracy: 0.75\n",
      "Batch 114 Train Accuracy: 0.25\n",
      "Batch 115 Train Accuracy: 1.0\n",
      "Batch 116 Train Accuracy: 0.75\n",
      "Batch 117 Train Accuracy: 0.5\n",
      "Batch 118 Train Accuracy: 0.5\n",
      "Batch 119 Train Accuracy: 0.5\n",
      "Batch 120 Train Accuracy: 0.5\n",
      "Epoch: 0, Batch: 120, Loss: 0.4147353768348694\n",
      "Batch 121 Train Accuracy: 1.0\n",
      "Batch 122 Train Accuracy: 0.75\n",
      "Batch 123 Train Accuracy: 0.75\n",
      "Batch 124 Train Accuracy: 0.75\n",
      "Batch 125 Train Accuracy: 1.0\n",
      "Batch 126 Train Accuracy: 0.75\n",
      "Batch 127 Train Accuracy: 0.25\n",
      "Batch 128 Train Accuracy: 0.75\n",
      "Batch 129 Train Accuracy: 0.75\n",
      "Batch 130 Train Accuracy: 0.25\n",
      "Epoch: 0, Batch: 130, Loss: 0.5808435678482056\n",
      "Batch 131 Train Accuracy: 0.5\n",
      "Batch 132 Train Accuracy: 0.75\n",
      "Batch 133 Train Accuracy: 1.0\n",
      "Batch 134 Train Accuracy: 0.5\n",
      "Batch 135 Train Accuracy: 0.5\n",
      "Batch 136 Train Accuracy: 0.75\n",
      "Batch 137 Train Accuracy: 0.75\n",
      "Batch 138 Train Accuracy: 0.5\n",
      "Batch 139 Train Accuracy: 0.75\n",
      "Batch 140 Train Accuracy: 0.25\n",
      "Epoch: 0, Batch: 140, Loss: 0.5216745138168335\n",
      "Batch 141 Train Accuracy: 0.5\n",
      "Batch 142 Train Accuracy: 0.5\n",
      "Batch 143 Train Accuracy: 0.75\n",
      "Batch 144 Train Accuracy: 0.25\n",
      "Batch 145 Train Accuracy: 0.5\n",
      "Batch 146 Train Accuracy: 0.25\n",
      "Batch 147 Train Accuracy: 0.75\n",
      "Batch 148 Train Accuracy: 0.75\n",
      "Batch 149 Train Accuracy: 0.5\n",
      "Batch 150 Train Accuracy: 0.5\n",
      "Epoch: 0, Batch: 150, Loss: 0.37670230865478516\n",
      "Batch 151 Train Accuracy: 0.25\n",
      "Batch 152 Train Accuracy: 0.75\n",
      "Batch 153 Train Accuracy: 0.75\n",
      "Batch 154 Train Accuracy: 0.5\n",
      "Batch 155 Train Accuracy: 0.75\n",
      "Batch 156 Train Accuracy: 0.25\n",
      "Batch 157 Train Accuracy: 0.5\n",
      "Batch 158 Train Accuracy: 1.0\n",
      "Batch 159 Train Accuracy: 1.0\n",
      "Batch 160 Train Accuracy: 0.5\n",
      "Epoch: 0, Batch: 160, Loss: 0.33846113085746765\n",
      "Batch 161 Train Accuracy: 0.5\n",
      "Batch 162 Train Accuracy: 0.0\n",
      "Batch 163 Train Accuracy: 0.3333333333333333\n",
      "Epoch 0 Average Training Loss: 0.34894057763058967, Avrage Training Accuraccy: 0.5675813008130082\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 4, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[3. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 1, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[5. 0. 0. 0. 0.]\n",
      " [2. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 2, predicted = 0\n",
      "[[8. 0. 0. 0. 0.]\n",
      " [2. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[11.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 1, predicted = 0\n",
      "Sample 1: true class = 1, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[13.  0.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Sample 0: true class = 1, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 2, predicted = 0\n",
      "Sample 3: true class = 1, predicted = 0\n",
      "[[14.  0.  0.  0.  0.]\n",
      " [ 7.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 1, predicted = 0\n",
      "Sample 2: true class = 2, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[16.  0.  0.  0.  0.]\n",
      " [ 8.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[20.  0.  0.  0.  0.]\n",
      " [ 8.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 1, predicted = 0\n",
      "Sample 2: true class = 2, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[22.  0.  0.  0.  0.]\n",
      " [ 9.  0.  0.  0.  0.]\n",
      " [ 4.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 2, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 1, predicted = 0\n",
      "[[23.  0.  0.  0.  0.]\n",
      " [11.  0.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 2, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 3, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[25.  0.  0.  0.  0.]\n",
      " [11.  0.  0.  0.  0.]\n",
      " [ 6.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 2, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[28.  0.  0.  0.  0.]\n",
      " [11.  0.  0.  0.  0.]\n",
      " [ 7.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 1, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[30.  0.  0.  0.  0.]\n",
      " [13.  0.  0.  0.  0.]\n",
      " [ 7.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[34.  0.  0.  0.  0.]\n",
      " [13.  0.  0.  0.  0.]\n",
      " [ 7.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[38.  0.  0.  0.  0.]\n",
      " [13.  0.  0.  0.  0.]\n",
      " [ 7.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 2, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 3, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[40.  0.  0.  0.  0.]\n",
      " [13.  0.  0.  0.  0.]\n",
      " [ 8.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Sample 0: true class = 1, predicted = 0\n",
      "Sample 1: true class = 1, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 1, predicted = 0\n",
      "[[40.  0.  0.  0.  0.]\n",
      " [17.  0.  0.  0.  0.]\n",
      " [ 8.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 1, predicted = 0\n",
      "Sample 1: true class = 1, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[41.  0.  0.  0.  0.]\n",
      " [20.  0.  0.  0.  0.]\n",
      " [ 8.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 1, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[43.  0.  0.  0.  0.]\n",
      " [22.  0.  0.  0.  0.]\n",
      " [ 8.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 1, predicted = 0\n",
      "[[45.  0.  0.  0.  0.]\n",
      " [24.  0.  0.  0.  0.]\n",
      " [ 8.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 2, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[48.  0.  0.  0.  0.]\n",
      " [24.  0.  0.  0.  0.]\n",
      " [ 9.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 2, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[50.  0.  0.  0.  0.]\n",
      " [25.  0.  0.  0.  0.]\n",
      " [10.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 2, predicted = 0\n",
      "[[53.  0.  0.  0.  0.]\n",
      " [25.  0.  0.  0.  0.]\n",
      " [11.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[56.  0.  0.  0.  0.]\n",
      " [26.  0.  0.  0.  0.]\n",
      " [11.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 1, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[59.  0.  0.  0.  0.]\n",
      " [27.  0.  0.  0.  0.]\n",
      " [11.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 2, predicted = 0\n",
      "Sample 3: true class = 1, predicted = 0\n",
      "[[61.  0.  0.  0.  0.]\n",
      " [28.  0.  0.  0.  0.]\n",
      " [12.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 1, predicted = 0\n",
      "Sample 2: true class = 2, predicted = 0\n",
      "Sample 3: true class = 3, predicted = 0\n",
      "[[62.  0.  0.  0.  0.]\n",
      " [29.  0.  0.  0.  0.]\n",
      " [13.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 1, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[65.  0.  0.  0.  0.]\n",
      " [30.  0.  0.  0.  0.]\n",
      " [13.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Sample 0: true class = 2, predicted = 0\n",
      "Sample 1: true class = 1, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 1, predicted = 0\n",
      "[[66.  0.  0.  0.  0.]\n",
      " [32.  0.  0.  0.  0.]\n",
      " [14.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 1, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 1, predicted = 0\n",
      "[[67.  0.  0.  0.  0.]\n",
      " [35.  0.  0.  0.  0.]\n",
      " [14.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 1, predicted = 0\n",
      "Sample 2: true class = 3, predicted = 0\n",
      "Sample 3: true class = 1, predicted = 0\n",
      "[[68.  0.  0.  0.  0.]\n",
      " [37.  0.  0.  0.  0.]\n",
      " [14.  0.  0.  0.  0.]\n",
      " [ 4.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[72.  0.  0.  0.  0.]\n",
      " [37.  0.  0.  0.  0.]\n",
      " [14.  0.  0.  0.  0.]\n",
      " [ 4.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 1, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[75.  0.  0.  0.  0.]\n",
      " [38.  0.  0.  0.  0.]\n",
      " [14.  0.  0.  0.  0.]\n",
      " [ 4.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 1, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 3, predicted = 0\n",
      "[[76.  0.  0.  0.  0.]\n",
      " [40.  0.  0.  0.  0.]\n",
      " [14.  0.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[79.  0.  0.  0.  0.]\n",
      " [41.  0.  0.  0.  0.]\n",
      " [14.  0.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 1, predicted = 0\n",
      "[[82.  0.  0.  0.  0.]\n",
      " [42.  0.  0.  0.  0.]\n",
      " [14.  0.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[86.  0.  0.  0.  0.]\n",
      " [42.  0.  0.  0.  0.]\n",
      " [14.  0.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[90.  0.  0.  0.  0.]\n",
      " [42.  0.  0.  0.  0.]\n",
      " [14.  0.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 1, predicted = 0\n",
      "Sample 2: true class = 2, predicted = 0\n",
      "Sample 3: true class = 1, predicted = 0\n",
      "[[91.  0.  0.  0.  0.]\n",
      " [44.  0.  0.  0.  0.]\n",
      " [15.  0.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 0, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 1, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[94.  0.  0.  0.  0.]\n",
      " [45.  0.  0.  0.  0.]\n",
      " [15.  0.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "Sample 0: true class = 3, predicted = 0\n",
      "Sample 1: true class = 0, predicted = 0\n",
      "Sample 2: true class = 0, predicted = 0\n",
      "Sample 3: true class = 0, predicted = 0\n",
      "[[97.  0.  0.  0.  0.]\n",
      " [45.  0.  0.  0.  0.]\n",
      " [15.  0.  0.  0.  0.]\n",
      " [ 6.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "Epoch 0, Avrage Test Accuraccy: 0.5914634146341463\n",
      "Batch 0 Train Accuracy: 0.25\n",
      "Epoch: 1, Batch: 0, Loss: 0.35129112005233765\n",
      "Batch 1 Train Accuracy: 1.0\n",
      "Batch 2 Train Accuracy: 0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Epoch Loop\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     validation(epoch)\n",
      "Cell \u001b[0;32mIn[51], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Epoch Loop\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "    validation(epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp(multi_class_confusion_matrix):\n",
    "    return np.diag(multi_class_confusion_matrix)\n",
    "\n",
    "def fp(multi_class_confusion_matrix):\n",
    "    return np.sum(multi_class_confusion_matrix, axis=0) - tp(multi_class_confusion_matrix)\n",
    "\n",
    "def fn(multi_class_confusion_matrix):\n",
    "    return np.sum(multi_class_confusion_matrix, axis=1) - tp(multi_class_confusion_matrix)\n",
    "\n",
    "def tn(multi_class_confusion_matrix):\n",
    "    total = np.sum(multi_class_confusion_matrix)\n",
    "    return total - (tp(multi_class_confusion_matrix) + fp(multi_class_confusion_matrix) + fn(multi_class_confusion_matrix))\n",
    "\n",
    "def precision(tp,fp):\n",
    "    return tp / (tp + fp) if (tp + fp).all() else np.nan\n",
    "\n",
    "def recall(tp,fn):\n",
    "    return tp / (tp + fn) if (tp + fn).all() else np.nan\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall).all() else np.nan\n",
    "\n",
    "def cm_metrics(cm):\n",
    "    tp_values = tp(cm)\n",
    "    fp_values = fp(cm)\n",
    "    fn_values = fn(cm)\n",
    "    tn_values = tn(cm)\n",
    "\n",
    "    class_metrics = {}\n",
    "\n",
    "    for i in range(len(cm)):\n",
    "        precision_values = precision(tp_values[i], fp_values[i])\n",
    "        recall_values = recall(tp_values[i], fn_values[i])\n",
    "        f1_values = f1_score(precision_values, recall_values)\n",
    "\n",
    "        class_metrics[i] = {\n",
    "            \"TP\": tp_values[i],\n",
    "            \"FP\": fp_values[i],\n",
    "            \"FN\": fn_values[i],\n",
    "            \"TN\": tn_values[i],\n",
    "            \"Precision\": precision_values,\n",
    "            \"Recall\": recall_values,\n",
    "            \"F1 Score\": f1_values\n",
    "        }\n",
    "\n",
    "    return class_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots & Metrics\n",
    "\n",
    "all_epoch_metrics = []\n",
    "for epoch, cm in enumerate(confusion_matrix_pr_epoch):  # confusion_matrices = list of matrices per epoch\n",
    "    class_metrics = cm_metrics(cm)\n",
    "    \n",
    "    # Attach epoch info\n",
    "    epoch_entry = {\n",
    "        \"epoch\": epoch,\n",
    "        \"metrics\": class_metrics  # class_metrics is already in {class_id: {...}} format\n",
    "    }\n",
    "    \n",
    "    all_epoch_metrics.append(epoch_entry)\n",
    "\n",
    "with open(\"epoch_metrics.json\", \"w\") as f:\n",
    "    json.dump(all_epoch_metrics, f, indent=2)\n",
    "\n",
    "epochs = list(range(EPOCHS))\n",
    "\n",
    "plt.plot(epochs, train_accuracies_pr_epoch, label='Train Accuracy')\n",
    "plt.plot(epochs, test_accuracies_pr_epoch, label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Train vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"learningcurve.png\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
